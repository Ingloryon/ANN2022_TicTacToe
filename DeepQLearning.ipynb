{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6e81632-6fc2-48e6-af2b-fdab3d24039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4072e629-a3f6-4519-9862-e5557db43313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.super()\n",
    "        self.hidden1 = nn.Linear(18, 100)\n",
    "        self.hidden2 = nn.Linear(100, 100)\n",
    "        self.output = nn.Linear(100, 9)\n",
    "    def forward(self, x): \n",
    "        x = x.view(-1,1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        return self.output(x)\n",
    "    def set_player_name(name):\n",
    "        self.player_name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e06b5-44bd-48ba-bdfa-4b652a274cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    " \n",
    "class ReplayBuffer(object): \n",
    " \n",
    "    def __init__(self, capacity): \n",
    "        self.memory = deque([],maxlen=capacity) \n",
    " \n",
    "    def push(self, *args): \n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args)) \n",
    " \n",
    "    def sample(self, batch_size): \n",
    "        return random.sample(self.memory, batch_size) \n",
    " \n",
    "    def __len__(self): \n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f7c44-1ebd-4ba7-ad56-e4c472d34052",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQTraining():\n",
    "    def __init__(self):\n",
    "        self.DISCOUNT_FACTOR = 0.99\n",
    "        self.BUFFER_SIZE = 10000\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.TARGET_NET_UPDATE_STEP = 500\n",
    "        self.LEARNING_RATE = 5e-4\n",
    "        self.NB_GAMES = 20000\n",
    "        self.MAX_GAME_LENGTH = 9\n",
    "        self.turns = np.array(['X','O'])\n",
    "        self.N_ACTIONS = 9\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.buffer = ReplayBuffer(self.BUFFER_SIZE)\n",
    "        self.agent1 = DeepQNetwork()\n",
    "        self.agent2 = DeepQNetwork()\n",
    "        self.optim = optim.Adam(self.agent1.parameters(), lr=self.LEARNING_RATE)\n",
    "        self.criterion = nn.HuberLoss()\n",
    "        self.env = TictactoeEnv()\n",
    "    \n",
    "    def get_other_player(self, model):\n",
    "        return \"X\" if model.player_name == \"O\" else \"O\"\n",
    "    \n",
    "    def choose_action(self, epsilon, env, model):\n",
    "        if random.random() <= epsilon:\n",
    "            return torch.tensor(random.randrange(self.N_ACTIONS))\n",
    "        else:\n",
    "            state = grid_to_state(env, model)\n",
    "            return model(state).max() # potential problem\n",
    "\n",
    "    def model_optimiser(self):\n",
    "        if len(self.buffer) < self.BATCH_SIZE: \n",
    "            return \n",
    "        \n",
    "        samples = self.buffer.sample(self.BATCH_SIZE)\n",
    "        #TODO \n",
    "        \n",
    "        output = self.agent1(samples)\n",
    "        loss = self.criterion(output, \n",
    "        \n",
    "    def grid_to_state(self, env,  model):\n",
    "        return torch.tensor([env.grid==env.player2value(model.player_name), grid==env.player2value(self.get_other_player(model.player_name))]) # Might be broken\n",
    "        \n",
    "    def simulate_game(self, env, opt_player, learner, epsilon_greedy):\n",
    "        for m in range(self.MAX_GAME_LENGTH):\n",
    "            if env.current_player == opt_player.player_name:\n",
    "                move = opt_player.act(env.grid)\n",
    "            else:\n",
    "                move = self.choose_action(epsilon_greedy, env.grid)\n",
    "            \n",
    "            prev_grid = env.grid.copy()\n",
    "            round_player = env.current_player\n",
    "            \n",
    "            if env.check_valid(move):\n",
    "                _, end, winner = env.step(move)\n",
    "                reward = env.reward(round_player)\n",
    "            else:\n",
    "                end = True\n",
    "                reward = -1\n",
    "                    \n",
    "            if learner.player_name == round_player:\n",
    "                self.buffer.push(grid_to_state(prev_grid), move, grid_to_state(env.grid.copy()), reward)\n",
    "                \n",
    "            if end:\n",
    "                return \n",
    "                    \n",
    "    def train(self, epsilon_greedy, adversary_epsilon):\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        for e in tqdm(range(NB_GAMES)):\n",
    "            self.env.reset()\n",
    "            self.agent1.player_name = turns[(e+1)%2]\n",
    "            opt_player = OptimalPlayer(adversary_epsilon, player_name=turns[e%2])\n",
    "            simulate_game(self.env, opt_player, self.agent1, epsilon_greedy)\n",
    "                                          \n",
    "            if (e+1)%self.TARGET_NET_UPDATE_STEP == 0:\n",
    "                model_optimiser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
