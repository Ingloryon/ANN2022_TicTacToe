{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6e81632-6fc2-48e6-af2b-fdab3d24039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea6801-d289-4135-9b2b-baace36b5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_player(self, player):\n",
    "    \"\"\"\n",
    "    Get the other opponent player name\n",
    "    :param player: the current player name\n",
    "    :return: the opponent player name\n",
    "    \"\"\"\n",
    "    return \"X\" if player.player_name == \"O\" else \"O\"\n",
    "def grid_to_state(self, env,  player):\n",
    "    \"\"\"\n",
    "    Convert the numpy grid to a tensor according to the definition in the handout\n",
    "    :param env: the current environement of the game\n",
    "    :param player: our current learner\n",
    "    \"\"\"\n",
    "    return torch.tensor([env.grid==env.player2value(player.player_name), grid==env.player2value(self.get_other_player(player.player_name))]) # Might be broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4072e629-a3f6-4519-9862-e5557db43313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Class representing our Neural Network to estimate the Q-values of each play\n",
    "    It is composed of two hidden layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.super()\n",
    "        self.hidden1 = nn.Linear(18, 100)\n",
    "        self.hidden2 = nn.Linear(100, 100)\n",
    "        self.output = nn.Linear(100, 9)\n",
    "    def forward(self, x): \n",
    "        x = x.view(-1,1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        return self.output(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e06b5-44bd-48ba-bdfa-4b652a274cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    " \n",
    "class ReplayBuffer(object): \n",
    " \n",
    "    def __init__(self, capacity): \n",
    "        self.memory = deque([],maxlen=capacity) \n",
    " \n",
    "    def push(self, *args): \n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args)) \n",
    " \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Get a random batch from the replay buffer\n",
    "        :param batch_size: the size of the batch we want to get\n",
    "        :return: the random batch\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size) \n",
    " \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the replay memory\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560d55a-11cd-45bc-9d96-d2873e823cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQPlayer():\n",
    "    def __init__(self, model): \n",
    "        self.model = model\n",
    "        \n",
    "    def act(self, epsilon, grid):\n",
    "        \"\"\"\n",
    "        Choose a move to perform for the given grid and the epsilon-greedy policy\n",
    "        :param epsilon: the epislon value used in the epsilon-greedy policy.\n",
    "        :param grid: the current state of the game\n",
    "        \"\"\"\n",
    "        if random.random() <= epsilon:\n",
    "            # Perform a random move\n",
    "            return torch.tensor(random.randrange(self.N_ACTIONS))\n",
    "        else:\n",
    "            # Choose the best action according to the ouptut of the estimation networks.\n",
    "            state = grid_to_state(grid, self.model)\n",
    "            return self.model(state).max() # potential problem\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f7c44-1ebd-4ba7-ad56-e4c472d34052",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQTraining():\n",
    "    def __init__(self):\n",
    "        # All constants needed in the optimisation \n",
    "        self.DISCOUNT_FACTOR = 0.99\n",
    "        self.BUFFER_SIZE = 10000\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.TARGET_NET_UPDATE_STEP = 500\n",
    "        self.LEARNING_RATE = 5e-4\n",
    "        self.NB_GAMES = 20000\n",
    "        self.MAX_GAME_LENGTH = 9\n",
    "        self.turns = np.array(['X','O'])\n",
    "        self.N_ACTIONS = 9\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Reset all training parameters to default \n",
    "        \"\"\"\n",
    "        self.buffer = ReplayBuffer(self.BUFFER_SIZE)\n",
    "        self.model = DeepQNetwork()\n",
    "        self.agent1 = DeepQPlayer(self.model)\n",
    "        self.agent2 = DeepQPlayer(self.model)\n",
    "        self.optim = optim.Adam(self.agent1.parameters(), lr=self.LEARNING_RATE)\n",
    "        self.criterion = nn.HuberLoss()\n",
    "        self.env = TictactoeEnv()\n",
    "    \n",
    "    \n",
    "    def model_optimiser(self):\n",
    "        \"\"\"\n",
    "        Optimise the parameter of the model according to the QDN algorithm\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.BATCH_SIZE: \n",
    "            return \n",
    "        \n",
    "        samples = self.buffer.sample(self.BATCH_SIZE)\n",
    "        #TODO \n",
    "        \n",
    "        output = self.agent1(samples)\n",
    "        loss = self.criterion(output, )\n",
    "        \n",
    "        \n",
    "    def simulate_game(self, env, opt_player, learner, epsilon_greedy):\n",
    "        \"\"\"\n",
    "        Simulate a full game with the different given player\n",
    "        :param env: the current environment of the game\n",
    "        :param opt_player: the player with the optimal policy\n",
    "        :param learner: our learner\n",
    "        :param epsilon_greedy: the value to use for epsilon in the epsilon-greedy policy. \n",
    "        \"\"\"\n",
    "    \n",
    "        for m in range(self.MAX_GAME_LENGTH):\n",
    "            # Iterate for the full game \n",
    "            if env.current_player == opt_player.player_name:\n",
    "                # If it is the turn of the optimal player, simply choose a move. \n",
    "                move = opt_player.act(env.grid)\n",
    "            else:\n",
    "                # If it is the turn of our learner, choose the best possible action\n",
    "                move = self.agent1.act(epsilon_greedy, env.grid)\n",
    "            \n",
    "            # Save current values to store them in the replay buffer later on\n",
    "            prev_grid = env.grid.copy()\n",
    "            round_player = env.current_player\n",
    "            \n",
    "            if env.check_valid(move):\n",
    "                # If the chosen move is valid, perform it and ovserve the reward\n",
    "                _, end, winner = env.step(move)\n",
    "                reward = env.reward(round_player)\n",
    "            else:\n",
    "                # If the chosen move is not valid, end the game and store a reward of -1\n",
    "                end = True\n",
    "                reward = -1\n",
    "                    \n",
    "            if learner.player_name == round_player:\n",
    "                # If it is our learner turn, store the current state, chosen action, reward and next state in the replay buffer\n",
    "                self.buffer.push(grid_to_state(prev_grid), move, grid_to_state(env.grid.copy()), reward)\n",
    "                \n",
    "            if end:\n",
    "                # End the simualtion of the game if the game is over\n",
    "                return \n",
    "                    \n",
    "    def train(self, epsilon_greedy, adversary_epsilon):\n",
    "        # Reset all the parameters for the start of the trianing\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        for e in tqdm(range(NB_GAMES)):\n",
    "            self.env.reset()\n",
    "            # set the corret player names\n",
    "            self.agent1.player_name = turns[(e+1)%2]\n",
    "            opt_player = OptimalPlayer(adversary_epsilon, player_name=turns[e%2])\n",
    "            # Simulate a game for the current epoch  \n",
    "            simulate_game(self.env, opt_player, self.agent1, epsilon_greedy(e))\n",
    "                                          \n",
    "            if (e+1)%self.TARGET_NET_UPDATE_STEP == 0:\n",
    "                # If we reach the update epoch, optimize the model\n",
    "                model_optimiser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
