{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a6e81632-6fc2-48e6-af2b-fdab3d24039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "0cea6801-d289-4135-9b2b-baace36b5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_player(player):\n",
    "    \"\"\"\n",
    "    Get the other opponent player name\n",
    "    :param player: the current player name\n",
    "    :return: the opponent player name\n",
    "    \"\"\"\n",
    "    return \"X\" if player == \"O\" else \"O\"\n",
    "def grid_to_state(grid,  env, player):\n",
    "    \"\"\"\n",
    "    Convert the numpy grid to a tensor according to the definition in the handout\n",
    "    :param env: the current environement of the game\n",
    "    :param player: our current learner\n",
    "    \"\"\"\n",
    "    return torch.tensor([grid==env.player2value[player.player], grid==env.player2value[get_other_player(player.player)]],dtype=torch.float).unsqueeze(0) # Might be broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "4072e629-a3f6-4519-9862-e5557db43313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Class representing our Neural Network to estimate the Q-values of each play\n",
    "    It is composed of two hidden layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(18, 100)\n",
    "        self.hidden2 = nn.Linear(100, 100)\n",
    "        self.output = nn.Linear(100, 9)\n",
    "    def forward(self, x): \n",
    "        x = x.flatten(1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        return self.output(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "b70e06b5-44bd-48ba-bdfa-4b652a274cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):  \n",
    "    def __init__(self, capacity):\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "        self.memory = deque([],maxlen=capacity) \n",
    " \n",
    "    def push(self, *args): \n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(self.Transition(*args)) \n",
    " \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Get a random batch from the replay buffer\n",
    "        :param batch_size: the size of the batch we want to get\n",
    "        :return: the random batch\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size) \n",
    " \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the replay memory\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "c560d55a-11cd-45bc-9d96-d2873e823cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQPlayer():\n",
    "    def __init__(self, model, player=\"\", N_ACTIONS=9): \n",
    "        self.model = model\n",
    "        self.player = player\n",
    "        \n",
    "    def act(self, epsilon, env,N_ACTIONS):\n",
    "        \"\"\"\n",
    "        Choose a move to perform for the given grid and the epsilon-greedy policy\n",
    "        :param epsilon: the epislon value used in the epsilon-greedy policy.\n",
    "        :param grid: the current state of the game\n",
    "        \"\"\"\n",
    "        if random.random() <= epsilon:\n",
    "            # Perform a random move\n",
    "            return torch.tensor(random.randrange(N_ACTIONS)).unsqueeze(-1)\n",
    "        else:\n",
    "            # Choose the best action according to the ouptut of the estimation networks.\n",
    "            state = grid_to_state(env.grid,env, self)\n",
    "            return self.model(state).argmax().unsqueeze(-1) # potential problem\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "6c6f7c44-1ebd-4ba7-ad56-e4c472d34052",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQTraining():\n",
    "    def __init__(self):\n",
    "        # All constants needed in the optimisation \n",
    "        self.DISCOUNT_FACTOR = 0.99\n",
    "        self.BUFFER_SIZE = 10000\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.TARGET_NET_UPDATE_STEP = 500\n",
    "        self.LEARNING_RATE = 5e-4\n",
    "        self.epoch = 20000\n",
    "        self.MAX_GAME_LENGTH = 9\n",
    "        self.turns = np.array(['X','O'])\n",
    "        self.N_ACTIONS = 9\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.TEST_STEP = 250\n",
    "        self.NB_TEST_GAME = 500\n",
    "        self.score_test_opt = []\n",
    "        self.score_test_rng = []\n",
    "        \n",
    "        self.AVG_STEP = 250\n",
    "        self.avg_reward = []\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Reset all training parameters to default \n",
    "        \"\"\"\n",
    "        self.buffer = ReplayBuffer(self.BUFFER_SIZE)\n",
    "        self.policy_network = DeepQNetwork()\n",
    "        self.target_network = DeepQNetwork()\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        self.agent1 = DeepQPlayer(self.policy_network)\n",
    "        self.agent2 = DeepQPlayer(self.policy_network)\n",
    "        self.optim = optim.Adam(self.policy_network.parameters(), lr=self.LEARNING_RATE)\n",
    "        self.criterion = nn.HuberLoss()\n",
    "        self.env = TictactoeEnv()\n",
    "        \n",
    "        self.score_test_opt = []\n",
    "        self.score_test_rng = []\n",
    "    \n",
    "    \n",
    "    def optimize_model(self):\n",
    "        \"\"\"\n",
    "        Optimise the parameter of the model according to the QDN algorithm\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.BATCH_SIZE: \n",
    "            return \n",
    "        \n",
    "        samples = self.buffer.sample(self.BATCH_SIZE)\n",
    "        \n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays. \n",
    "        # This comes from the PyTorch tutorial\n",
    "        # Go from [Transition(state='state', action='a', next_state='state2', reward=1)] \n",
    "        # to Transition(state=('state',), action=('a',), next_state=('state2',), reward=(1,))\n",
    "        batch = buffer.Transition(*zip(*samples))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_network(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0].detach()\n",
    "        \n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.DISCOUNT_FACTOR) + reward_batch\n",
    "        loss = self.criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        # peform weight updates\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "    def test(self, cur_seed):\n",
    "        \"\"\"\n",
    "        Test our learned against a random player and an optimal player\n",
    "        \"\"\"\n",
    "        # Define default values\n",
    "        nb_win_opt = 0\n",
    "        nb_lose_opt = 0\n",
    "        nb_win_rng = 0\n",
    "        nb_lose_rng = 0\n",
    "        env_test = TictactoeEnv()\n",
    "        \n",
    "        def test_game(opp_eps, epoch, cur_seed):\n",
    "            \"\"\"\n",
    "            Run a test game \n",
    "            :param opp_eps: the epsilon for the opponent optimal player\n",
    "            :param epoch: the current epoch\n",
    "            :returns: tuple (a, b) with a=1 if our learner wins and b=1 if he looses\n",
    "            \"\"\"\n",
    "            random.seed(cur_seed)\n",
    "            env_test.reset()\n",
    "            q_player = self.agent1\n",
    "            player_opt = OptimalPlayer(epsilon=opp_eps, player=self.turns[epoch%2])\n",
    "            \n",
    "            # When testing we do not update Qvals and set epsilon_learner parameter at 0.0 (no random exploration action)\n",
    "            self.simulate_game(env_test, player_opt, q_player, 0.0, in_test = True)\n",
    "            return (1 if env_test.winner == q_player.player else 0), (1 if env_test.winner == player_opt.player else 0)\n",
    "        \n",
    "        for ep in range(0, self.NB_TEST_GAME):\n",
    "            # Simulate games against optimal player and random player\n",
    "            cur_seed += 1\n",
    "            win_opt, lose_opt = test_game(0.0, ep, cur_seed)\n",
    "            cur_seed += 1\n",
    "            win_rng, lose_rng = test_game(1.0, ep, cur_seed)\n",
    "            \n",
    "            # Update the value depending on win or loose\n",
    "            nb_win_opt += win_opt\n",
    "            nb_lose_opt += lose_opt\n",
    "            nb_win_rng += win_rng\n",
    "            nb_lose_rng += lose_rng\n",
    "        \n",
    "        # Store M_opt and M_rand \n",
    "        self.score_test_opt.append((nb_win_opt-nb_lose_opt)/self.NB_TEST_GAME)\n",
    "        self.score_test_rng.append((nb_win_rng-nb_lose_rng)/self.NB_TEST_GAME)\n",
    "        \n",
    "    def simulate_game(self, env, opt_player, learner, epsilon_greedy, in_test = False):\n",
    "        \"\"\"\n",
    "        Simulate a full game with the different given player\n",
    "        :param env: the current environment of the game\n",
    "        :param opt_player: the player with the optimal policy\n",
    "        :param learner: our learner\n",
    "        :param epsilon_greedy: the value to use for epsilon in the epsilon-greedy policy. \n",
    "        \"\"\"\n",
    "    \n",
    "        for m in range(self.MAX_GAME_LENGTH):\n",
    "            # Iterate for the full game \n",
    "            if env.current_player == opt_player.player:\n",
    "                # If it is the turn of the optimal player, simply choose a move. \n",
    "                move = opt_player.act(env.grid)\n",
    "                move_int = move\n",
    "            else:\n",
    "                # If it is the turn of our learner, choose the best possible action\n",
    "                move = self.agent1.act(epsilon_greedy, env, self.N_ACTIONS)\n",
    "                move_int = move.item()\n",
    "            \n",
    "            # Save current values to store them in the replay buffer later on\n",
    "            prev_grid = env.grid.copy()\n",
    "            round_player = env.current_player\n",
    "            if env.check_valid(move_int):\n",
    "                # If the chosen move is valid, perform it and ovserve the reward\n",
    "                _, end, winner = env.step(move_int)\n",
    "                reward = env.reward(round_player)\n",
    "            else:\n",
    "                # If the chosen move is not valid, end the game and store a reward of -1\n",
    "                end = True\n",
    "                reward = -1\n",
    "                    \n",
    "            if learner.player == round_player and not in_test:\n",
    "                # If it is our learner turn, store the current state, chosen action, reward and next state in the replay buffer\n",
    "                # We set the next state to None if the game is ended. \n",
    "                self.buffer.push(grid_to_state(prev_grid,env,learner), move, None if end else grid_to_state(env.grid.copy(),env,learner), torch.tensor(reward).unsqueeze(-1))\n",
    "                \n",
    "            if end:\n",
    "                # End the simualtion of the game if the game is over\n",
    "                return \n",
    "                    \n",
    "    def train(self, epsilon_greedy, adversary_epsilon, run_test = True, seed=42):\n",
    "        # set seed for reproducible results\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # Reset all the parameters for the start of the trianing\n",
    "        self.reset_parameters()\n",
    "        acc_rew = 0\n",
    "        for e in tqdm(range(self.epoch)):\n",
    "            self.env.reset()\n",
    "            \n",
    "            seed+=1\n",
    "            random.seed(seed)\n",
    "            \n",
    "            if((e+1)%self.TEST_STEP == 0):\n",
    "                self.test(seed)\n",
    "                \n",
    "            if((e+1)%self.AVG_STEP == 0):\n",
    "                self.avg_reward.append(acc_rew/self.AVG_STEP)\n",
    "                acc_rew = 0\n",
    "            \n",
    "            # set the corret player names\n",
    "            self.agent1.player = self.turns[(e+1)%2]\n",
    "            opt_player = OptimalPlayer(adversary_epsilon, player=self.turns[e%2])\n",
    "            \n",
    "            # Simulate a game for the current epoch  \n",
    "            self.simulate_game(self.env, opt_player, self.agent1, epsilon_greedy(e))\n",
    "            \n",
    "            acc_rew += self.env.reward(self.agent1)\n",
    "            \n",
    "            # Perform one step of optimization (on the policy network)\n",
    "            self.optimize_model()\n",
    "                                          \n",
    "            if (e+1)%self.TARGET_NET_UPDATE_STEP == 0:\n",
    "                # If we reach the update epoch, update the parameters in the target networ.\n",
    "                self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "                \n",
    "                \n",
    "    def plot_mean_reward_during_training(self):\n",
    "        \"\"\"\n",
    "        Make a plot with the mean reward gotten during training\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.plot(range(0, self.epoch, self.AVG_STEP), self.avg_reward, 'b', lw=4)\n",
    "        plt.xlabel('Number of games played', fontsize= 20)\n",
    "        plt.ylabel('Mean reward over {} games'.format(self.AVG_STEP), fontsize = 20)\n",
    "        plt.title('Evolution of mean reward (every {} games played) of the learner'.format(self.AVG_STEP), fontsize = 20)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_mopt_mrng_during_training(self):\n",
    "        \"\"\"\n",
    "        Make a plot with the performance against optimal and random players\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.plot(range(0, self.epoch, self.TEST_STEP), self.score_test_opt, color='blue', lw=4, label=\"Against Optimal Player\")\n",
    "        plt.plot(range(0, self.epoch, self.TEST_STEP), self.score_test_rng, color='orange', lw=4, label=\"Against Random Player\")\n",
    "        plt.xlabel('Number of games played', fontsize= 20)\n",
    "        plt.ylabel('Mean reward over {} games'.format(self.TEST_STEP), fontsize = 20)\n",
    "        plt.legend(loc=2)\n",
    "        plt.title('Evolution of mean reward (every {} games played) of the learner'.format(self.TEST_STEP), fontsize = 20)\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6f4cb-de5a-40fd-895e-9597d4669e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 19481/20000 [03:54<00:02, 184.87it/s]"
     ]
    }
   ],
   "source": [
    "trainer = DeepQTraining()\n",
    "trainer.train(lambda e: 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fd625-8652-44dc-98e6-f1224d80e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_mean_reward_during_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7fb73-c53a-4a8d-9c91-21a64ffbab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_mopt_mrng_during_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713d83d-fc15-4960-9a17-d014c8bfb8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9863b-513b-4aa9-a41d-35fe678b2ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b45cb-fee7-40d1-8d1c-5b73905995ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef127f-3346-4f9f-81ef-62a1b5ad9cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
