{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1add4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from QLearning_env import QPlayer, QTraining, get_state_best_Q\n",
    "from utils import plots_several_trainings, plots_several_trainings_subfigures, plot_game_heatmaps\n",
    "# from utils import action_to_key, grid_to_string\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65301131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████████████████████████████████████████████████████▍                                                                                                                      | 6746/20000 [01:17<01:01, 216.36it/s]"
     ]
    }
   ],
   "source": [
    "# Part 2.1 (question 1)\n",
    "eps_policy = lambda ep: 0.1\n",
    "\n",
    "training = QTraining(eps_policy)\n",
    "training.train(0.5, run_test= True)\n",
    "\n",
    "training.plot_mean_reward_during_training()\n",
    "training.plot_mopt_mrng_during_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09341d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.1.1\n",
    "n_stars = [1, 5000, 10000, 15000, 25000, 40000]\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8\n",
    "\n",
    "values = []\n",
    "values_mopt_mrng = []\n",
    "names = []\n",
    "epochs = 0\n",
    "avg_step = 0\n",
    "\n",
    "for n_star in n_stars:\n",
    "    epsilon_greedy_policy = lambda ep: max(epsilon_min, epsilon_max*(1-ep/n_star))\n",
    "    training2 = QTraining(epsilon_greedy_policy)\n",
    "    training2.train(0.5, run_test=True)\n",
    "    epochs = training2.epoch\n",
    "    avg_step = training2.avg_step\n",
    "    \n",
    "    values.append(training2.avg_reward)\n",
    "    values_mopt_mrng.append([training2.score_test_opt, training2.score_test_rng])\n",
    "    names.append(f\"$n^*=${n_star}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (question 2)\n",
    "plots_several_trainings(values, names, avg_step, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5fddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_several_trainings_subfigures(values, names, avg_step, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef64841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (question 3)\n",
    "plots_several_trainings_subfigures(values_mopt_mrng, names, training2.test_step, epochs, mopt_mrng=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511cf82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DEBUG) Plotting just one of the curves\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "xs = range(0, epochs, avg_step)\n",
    "\n",
    "plt.plot(xs, values[2], label=names[2], lw=2)\n",
    "\n",
    "plt.xlabel('Number of games played', fontsize= 20)\n",
    "plt.ylabel('Mean reward over {} games'.format(avg_step), fontsize = 20)\n",
    "plt.title('Evolution of mean reward (every {} games played) of the learner'.format(avg_step), fontsize = 20)\n",
    "plt.grid()\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DEBUG) Avg reward\n",
    "\n",
    "for elem in values:\n",
    "    print(np.mean(elem))\n",
    "    \n",
    "# ==> so it seems the lower n_star the faster we will learn which does not make intuitive sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9080919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.1.2\n",
    "\n",
    "# TODO: how do we want to select the \"best\" n_star? Atm just one with best average reward during whole training\n",
    "n_star_best = n_stars[np.argmax(np.mean(values, axis=1))]\n",
    "\n",
    "epsilon_opts = [0.0, 0.1, 0.3, 0.5, 0.8, 1.0]\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8\n",
    "epsilon_greedy_policy = lambda ep: max(epsilon_min, epsilon_max*(1-ep/n_star_best))\n",
    "\n",
    "values = []\n",
    "values_mopt_mrng = []\n",
    "names = []\n",
    "epochs = 0\n",
    "avg_step = 0\n",
    "\n",
    "for epsilon_opt in epsilon_opts:\n",
    "    training2 = QTraining(epsilon_greedy_policy)\n",
    "    training2.train(epsilon_opt, run_test=True)\n",
    "    epochs = training2.epoch\n",
    "    avg_step = training2.avg_step\n",
    "    \n",
    "    values.append(training2.avg_reward)\n",
    "    values_mopt_mrng.append([training2.score_test_opt, training2.score_test_rng])\n",
    "    names.append(f\"$epsilon_opt=${epsilon_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (question 4)\n",
    "plots_several_trainings(values, names, avg_step, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_several_trainings_subfigures(values, names, avg_step, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f7b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_several_trainings_subfigures(values_mopt_mrng, names, training2.test_step, epochs, mopt_mrng=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7987c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DEBUG) Remarque: on dirait que contre Opt(0), il y a un moment ou il trouve une technique parfaite pour toujours faire Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (question 5)\n",
    "get_max_Mopt_Mrng_for_epsilon(values_mopt_mrng,epsilon_opts, \"epsilon-opt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d6926",
   "metadata": {},
   "source": [
    "By looking at the figures above we can see we achieve 0 mean reward (on testing) in each case except when training against fully random adversary. It seems the best $\\epsilon_{opt}$ is therefore 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7435703-8701-4e6e-b8cf-d18354d66779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (question 7)\n",
    "\n",
    "eps = [0.5]\n",
    "#eps = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "\n",
    "values = []\n",
    "values_mopt_mrng = []\n",
    "names = []\n",
    "epochs = 0\n",
    "avg_step = 0\n",
    "\n",
    "for ep in eps:\n",
    "    epsilon_greedy_policy = lambda ep: ep\n",
    "    training = QTraining(epsilon_greedy_policy)\n",
    "    training.train(ep, run_test=True, self_learning = True)\n",
    "    epochs = training.epoch\n",
    "    avg_step = training.avg_step\n",
    "    \n",
    "    values.append(training.avg_reward)\n",
    "    values_mopt_mrng.append([training2.score_test_opt, training2.score_test_rng])\n",
    "    names.append(f\"$epsilon=${ep}\")\n",
    "    training.plot_mean_reward_during_training()\n",
    "    training.plot_mopt_mrng_during_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c28c02-e28b-4a00-b72a-5bcb5daae7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stars = [1, 5000, 10000, 15000, 25000, 40000]\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8\n",
    "\n",
    "values = []\n",
    "values_mopt_mrng = []\n",
    "names = []\n",
    "epochs = 0\n",
    "avg_step = 0\n",
    "\n",
    "for n_star in n_stars:\n",
    "    epsilon_greedy_policy = lambda ep: max(epsilon_min, epsilon_max*(1-ep/n_star))\n",
    "    training2 = QTraining(epsilon_greedy_policy)\n",
    "    training2.train(0.5, run_test=True, self_learning=True)\n",
    "    epochs = training2.epoch\n",
    "    avg_step = training2.avg_step\n",
    "    \n",
    "    values.append(training2.avg_reward)\n",
    "    values_mopt_mrng.append([training2.score_test_opt, training2.score_test_rng])\n",
    "    names.append(f\"$n^*=${n_star}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9265549b-669b-4d15-a4f4-e4e65881228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_several_trainings(values, names, avg_step, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a7461-4a97-4f5e-b2bd-50bfb1ec3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_several_trainings_subfigures(values, names, avg_step, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1861355-6b8d-479a-8554-6a3e288b4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (question 3)\n",
    "plots_several_trainings_subfigures(values_mopt_mrng, names, training2.test_step, epochs, mopt_mrng=True)\n",
    "# TODO: compare in a plot best eps fixed with best epsilon decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a36873b-8cd8-4129-9a3b-bbd7b1c3b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (question 9)\n",
    "get_max_Mopt_Mrng_for_epsilon(values_mopt_mrng, n_stars, \"n_star\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf41fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (question 10)\n",
    "\n",
    "states = [\"---------\", \"XX-OO----\", \"X--O-X-O-\"]\n",
    "titles = [\"Starting board\", \"Winning position\", \"Tactical move\"]\n",
    "\n",
    "# situation1: Starting board\n",
    "\"\"\"\n",
    "- - -\n",
    "- - -\n",
    "- - -\n",
    "\"\"\"\n",
    "# situation2: Winning position\n",
    "\"\"\" \n",
    "X X -\n",
    "O O -\n",
    "- - -\n",
    "\"\"\"\n",
    "\n",
    "# situation3: Tactical move\n",
    "\"\"\" \n",
    "X - -\n",
    "O - X\n",
    "- O -\n",
    "\"\"\"\n",
    "\n",
    "# Train the self-learners to obtain final Q-values\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8\n",
    "n_star = 10000 # TODO change to the best n_start\n",
    "training2 = QTraining(lambda ep: max(epsilon_min, epsilon_max*(1-ep/n_star)))\n",
    "training2.train(0.5, run_test=False, self_learning=True)\n",
    "\n",
    "plot_game_heatmaps(states, training2.Q_vals, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb87ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
